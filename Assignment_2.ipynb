{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1955fa46",
   "metadata": {},
   "source": [
    "# Image Classification using Convolution Neural Network\n",
    "Multi-Label Image Classification Model (1 input - multiple outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40158a55",
   "metadata": {},
   "source": [
    "# I. Setup and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961dd409",
   "metadata": {},
   "source": [
    "### install dependency and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e621f42",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install opencv-python #to remove bullshit image\n",
    "\n",
    "!pip install tensorflow \n",
    "\n",
    "!pip install tensorflow-macos tensorflow-metal #these two packages allow tensorflow to run on MacOS\n",
    "\n",
    "install miniforge in terminal using https://www.youtube.com/watch?v=w2qlou7n7MA (for mac users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc1ad1",
   "metadata": {},
   "source": [
    "In mac, we can't use GPU so we need to install miniforge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238b8de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6a7f7",
   "metadata": {},
   "source": [
    "Don't use version above this since imghdr package I'm using might be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d894ee",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3da801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os # to navigate through file structure\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import layers # for data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a50ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') #grab all available gpus on our machine\n",
    "for gpu in gpus: # limit the memory growth\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb11dc6",
   "metadata": {},
   "source": [
    "- avoid OOM errors\n",
    "- limit the tenserflow so it won't use all of the vram on our gpu when loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6633d90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f0ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d26fd0",
   "metadata": {},
   "source": [
    "### remove dogy image\n",
    "remove corrupted, mislable image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb25831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import imghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74445102",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Furniture_Data_copy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07ec2c",
   "metadata": {},
   "source": [
    "- First, manually delete any image that has the size below 10 KB (it's too small)\n",
    "- Next, remove dodgy images using loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5eb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_exts = ['jpeg','jpg', 'bmp', 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2077f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_count = 0 # initialize counter\n",
    "# import os\n",
    "\n",
    "# for image_class in os.listdir(data_dir): # loop through each folder and their sub-folder to count using os\n",
    "#     class_path = os.path.join(data_dir, image_class)\n",
    "#     if os.path.isdir(class_path):  # check if it's a directory\n",
    "#         for image_sub in os.listdir(class_path):\n",
    "#             sub_path = os.path.join(class_path, image_sub)\n",
    "#             if os.path.isdir(sub_path):  # check if it's a directory\n",
    "#                 for image in os.listdir(sub_path):\n",
    "#                     image_path = os.path.join(sub_path,image)\n",
    "#                     # print(image)\n",
    "#                     image_count += 1\n",
    "            \n",
    "# print(\"Total images:\", image_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b352a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for image_class in os.listdir(data_dir): # loop through each folder and their sub-folder to count using os\n",
    "#     class_path = os.path.join(data_dir, image_class)\n",
    "#     if os.path.isdir(class_path):  # check if it's a directory\n",
    "#         for image_sub in os.listdir(class_path):\n",
    "#             sub_path = os.path.join(class_path, image_sub)\n",
    "#             if os.path.isdir(sub_path):  # check if it's a directory\n",
    "#                 for image in os.listdir(sub_path):\n",
    "#                     image_path = os.path.join(sub_path,image)\n",
    "#                     try: \n",
    "#                         img = cv2.imread(image_path) # read image information\n",
    "#                         tip = imghdr.what(image_path)\n",
    "#                         if tip not in image_exts: \n",
    "#                             print('Image not in ext list {}'.format(image_path))\n",
    "#                             #os.remove(image_path)\n",
    "#                     except Exception as e: \n",
    "#                         print('Issue with image {}'.format(image_path))\n",
    "#                         # os.remove(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899a231",
   "metadata": {},
   "source": [
    "There is no dodgy image to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4757ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(os.path.join(data_dir,'beds','Asian','2537asian-platform-beds.jpg'))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2be4a",
   "metadata": {},
   "source": [
    "This show the information of the image: 224 pixels high, 224 pixels wide, 3 = color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4e0bc",
   "metadata": {},
   "source": [
    "### Load + Preprocessing data\n",
    "For such a large dataset, an on-the-fly loading mechanism is recommanded rather than loading all data into memory at once. \n",
    "\n",
    "ImageDataGenerator is a input pipeline, you can use it to feed data directly to the model without storing huge ass data to the memory\n",
    "\n",
    "https://viblo.asia/p/xay-dung-input-pipeline-cho-du-lieu-dang-anh-voi-tensoflow-keras-maGK7GnOKj2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdd0b4",
   "metadata": {},
   "source": [
    "create data pipeline (load + preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d2fe219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21901 images belonging to 3 classes.\n",
      "Found 14600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( # template to call data (preprocessing)\n",
    "    rescale=1./255, #scaling\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.4) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', # this parameter affect the label(output) shape\n",
    "    subset='training') # set as training data\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "counter = Counter(train_generator.classes)\n",
    "max_val = float(max(counter.values()))\n",
    "class_weights = {class_id: max_val/num_images for class_id, num_images in counter.items()}\n",
    "\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3719e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_item = train_generator.__getitem__(0) #check the first item\n",
    "first_item[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afeb6d",
   "metadata": {},
   "source": [
    "### Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d660bb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Get all labels from the dataset\n",
    "# all_labels = []\n",
    "# for _, labels in train_generator:\n",
    "#     all_labels.extend(labels)\n",
    "\n",
    "# # Convert to numpy array for manipulation\n",
    "# all_labels = np.array(all_labels)\n",
    "\n",
    "# # Count the unique labels and their occurrences\n",
    "# unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "\n",
    "# # Create a dictionary to map labels to class names\n",
    "# label_dict = {0: 'bed (0)', 1: 'chair (1)', 2: 'dresser (2)'}\n",
    "\n",
    "# # Replace labels with class names\n",
    "# class_names = [label_dict[label] for label in unique_labels]\n",
    "\n",
    "# # Plotting the distribution\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Use a color palette from seaborn\n",
    "# palette = sns.color_palette(\"hls\", len(unique_labels))\n",
    "\n",
    "# bars = plt.bar(class_names, counts, color=palette)\n",
    "\n",
    "# # Add counts on top of each bar\n",
    "# for bar in bars:\n",
    "#     yval = bar.get_height()\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, yval, ha='center', va='bottom')\n",
    "\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Distribution of Classes')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2902662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all labels from the dataset\n",
    "# all_labels = []\n",
    "# for _, labels in data:\n",
    "#     all_labels.extend(labels.numpy())\n",
    "\n",
    "# # Convert to numpy array for manipulation\n",
    "# all_labels = np.array(all_labels)\n",
    "\n",
    "# # Count the unique labels and their occurrences\n",
    "# unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "\n",
    "# # Create a dictionary to map labels to class names\n",
    "# label_dict = {0: 'bed (0)', 1: 'chair (1)', 2: 'dresser (2)'}\n",
    "\n",
    "# # Replace labels with class names\n",
    "# class_names = [label_dict[label] for label in unique_labels]\n",
    "\n",
    "# # Plotting the distribution\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Use a color palette from seaborn\n",
    "# palette = sns.color_palette(\"hls\", len(unique_labels))\n",
    "\n",
    "# bars = plt.bar(class_names, counts, color=palette)\n",
    "\n",
    "# # Add counts on top of each bar\n",
    "# for bar in bars:\n",
    "#     yval = bar.get_height()\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, yval, ha='center', va='bottom')\n",
    "\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Distribution of Classes')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc5fd2",
   "metadata": {},
   "source": [
    "The classes are highly imbalanced\n",
    "\n",
    "**Solution:** I will use the **class_weights** argument in model.fit (make the model learn more from the minority class).\n",
    "\n",
    "**Other sugestions:**\n",
    "- Data augmentation:  make the most of the minority class (I can't find a way to do it with image_dataset_from_directory or flow_from_directory)\n",
    "- Oversampling: they are bad for image data since it require images to be reshaped into 2D which would lose its information\n",
    "\n",
    "https://stackoverflow.com/questions/53666759/use-smote-to-oversample-image-data\n",
    "\n",
    "https://stackoverflow.com/questions/41648129/balancing-an-imbalanced-dataset-with-keras-image-generator\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ae68",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    " \n",
    " To avoid data leak, data augmentation should be performed before splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d85af8",
   "metadata": {},
   "source": [
    "test the Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05ed9e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Augmenting the images\n",
    "# data_augmentation = tf.keras.Sequential(\n",
    "#     [\n",
    "#         tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "#         tf.keras.layers.RandomRotation(0.1),\n",
    "#         tf.keras.layers.RandomZoom(0.2)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# #test the Data Augmentation on an image in the dataset\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, _ in data.take(2): # take an image\n",
    "#     for i in range(9): # make 9 versions of it\n",
    "#         augmented_images = data_augmentation(images)\n",
    "        \n",
    "#         ax = plt.subplot(3, 3, i + 1)# plot\n",
    "#         plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf3d9e",
   "metadata": {},
   "source": [
    " we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b85c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#         rotation_range=40,\n",
    "#         width_shift_range=0.2,\n",
    "#         height_shift_range=0.2,\n",
    "#         rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b2e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2deab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe798847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b1993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3f5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cw = tf.data.Dataset.from_tensor_slices((images, labels, class_weight=='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4ec83",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a55f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e997ef",
   "metadata": {},
   "source": [
    "we have 1141 batches, each of them contain 32 images (in total there are 36502 images of 3 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8a4cc",
   "metadata": {},
   "source": [
    "decide the amount of train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "394733b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(len(data)*.6)\n",
    "# val_size = int(len(data)*.2)\n",
    "# test_size = int(len(data)*.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1209594",
   "metadata": {},
   "source": [
    "we gonna use \"take\" and \"skip\" in tensorflow to split whole dataset into 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e61014d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = scaled_data.take(train_size) # data already has x,y seperately\n",
    "# val = scaled_data.skip(train_size).take(val_size)\n",
    "# test = scaled_data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6dcc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainlen = len(train)\n",
    "# vallen = len(val)\n",
    "# testlen = len(test)\n",
    "# print (\"number of patches in train data: \", trainlen)\n",
    "# print (\"number of patches in val data: \", vallen)\n",
    "# print (\"number of patches in test data: \", testlen)\n",
    "# print (\"\\neach patch has contains 32 images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68111fba",
   "metadata": {},
   "source": [
    "Then we create batch to access to x,y of each sub-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a34b08",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f5233d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # the model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout #layer of that model\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17c8d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential() # define a model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03362a2",
   "metadata": {},
   "source": [
    "add layers to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "169f241c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Convolutional layers\n",
    "# model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(IMG_WIDTH,IMG_HEIGHT,3))) # specify the input shape in the first layer\n",
    "# model.add(MaxPooling2D()) \n",
    "# model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "# model.add(MaxPooling2D())\n",
    "# model.add(Conv2D(16, (3,3), 1, activation='relu'))\n",
    "# model.add(MaxPooling2D())\n",
    "\n",
    "# # Flatten layer to transition from convolutional to fully connected layers\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # Fully connected layers\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# # Output (3 category = 3 outputs)\n",
    "# model.add(Dense(3, activation='softmax')) # for multiclass classification, use Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d23dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadeec52",
   "metadata": {},
   "source": [
    "Use \"SparseCategoricalCrossentropy()\" crossentropy loss function when:\n",
    "- there are two or more label classes.\n",
    "- labels are provided as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a71ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e8730",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eb10edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir='logs' # create a logs folder (for check point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1656f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir) # (for check point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7eec2e",
   "metadata": {},
   "source": [
    "**handle imbalance** by class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a6ddeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# histCNN = model.fit(train_generator, \n",
    "#                     epochs=1, \n",
    "#                     validation_data=validation_generator,\n",
    "#                     class_weight = class_weights, \n",
    "#                     callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d611ae",
   "metadata": {},
   "source": [
    "## Duong's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "602ceabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, Dense, BatchNormalization, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a89a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:16:18.644868: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-05-10 09:16:18.644901: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-10 09:16:18.644907: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-10 09:16:18.644926: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-10 09:16:18.644942: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the pretained model\n",
    "pretrained_model = VGG19(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86131af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './Models/vgg19-model.weights.h5'\n",
    "model_callback = ModelCheckpoint(model_path,\n",
    "                                 save_weights_only=True,\n",
    "                                 monitor=\"val_accuracy\",\n",
    "                                 save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-10)\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bfc7fc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:16:20.816274: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/quangdong/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  5/685\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:43\u001b[0m 2s/step - accuracy: 0.3935 - loss: 1.8738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m historyDuong \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     22\u001b[0m     train_generator,\n\u001b[1;32m     23\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_generator),\n\u001b[1;32m     24\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator,\n\u001b[1;32m     25\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(validation_generator),\n\u001b[1;32m     26\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     27\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     28\u001b[0m         early_stopping,\n\u001b[1;32m     29\u001b[0m         tensor_board,\n\u001b[1;32m     30\u001b[0m         model_callback,\n\u001b[1;32m     31\u001b[0m         reduce_lr\n\u001b[1;32m     32\u001b[0m     ]\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = pretrained_model.input\n",
    "\n",
    "x = Dense(128, activation='relu')(pretrained_model.output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.45)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.45)(x)\n",
    "\n",
    "\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historyDuong = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    epochs=1,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        tensor_board,\n",
    "        model_callback,\n",
    "        reduce_lr\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ef85c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#hhist = model.fit(train, epochs=1, validation_data=val, sample_weight='balanced', callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# plt.plot(histCNN.history['accuracy'], color='teal', label='accuracy')\n",
    "# plt.plot(histCNN.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "# fig.suptitle('Accuracy', fontsize=20)\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "# fig = plt.figure()\n",
    "# plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "# plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "# fig.suptitle('Loss', fontsize=20)\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre = Precision()\n",
    "# re = Recall()\n",
    "# acc = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dbd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in test.as_numpy_iterator(): \n",
    "#     X, y = batch\n",
    "#     yhat = model.predict(X)\n",
    "#     pre.update_state(y, yhat)\n",
    "#     re.update_state(y, yhat)\n",
    "#     acc.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330dfce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(pre.result(), re.result(), acc.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76756863",
   "metadata": {},
   "source": [
    "## Hyper parameters tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58351e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definr some Hyper parameters\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a33951",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e471a",
   "metadata": {},
   "source": [
    "let pick a random image on the internet to test (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05792cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread('Screenshot 2024-05-09 at 11.41.28.PNG')\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53206b91",
   "metadata": {},
   "source": [
    "resize that image so it could fit into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcc081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resize = tf.image.resize(img, (256,256))\n",
    "# plt.imshow(resize.numpy().astype(int))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e421228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yhat = model.predict(np.expand_dims(resize/255, 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195beaa",
   "metadata": {},
   "source": [
    "np.expand_dims to put the image into an extra array (model doesn't work with 1 image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad777e8f",
   "metadata": {},
   "source": [
    "The images are now load, shuffel, resize, set batch size = 32 and label:\n",
    "- 0 is bed\n",
    "- 1 is chair\n",
    "- 2 is dresser\n",
    "- 3 is lamp and \n",
    "- 4 is sofa\n",
    "- 5 is table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128439c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c7699ed",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775b9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb7f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
